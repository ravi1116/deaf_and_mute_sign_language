WELCOME TO SIH 
CONGRATS IF YOU MADE IT TILL HERE COZ IF YOU ARE SEEING THIS IT MEAN WE WON THE FIRST LEVEL AND WE WE ARE AT LEVEL 2 OF THE SIH 
ğŸš€ Roadmap for MAITRI

Phase 1: Foundation (Day 1â€“2)

Goal: Set up base environment + core pipeline.
	â€¢	âœ… Choose tech stack:
	â€¢	Backend: Python (FastAPI/Flask).
	â€¢	Frontend: Streamlit or simple React web app.
	â€¢	Models: Hugging Face models for speech-to-text + emotion detection.
	â€¢	âœ… Collect sample datasets for testing:
	â€¢	Audio emotions: RAVDESS, CREMA-D.
	â€¢	Facial emotions: FER2013, AffectNet (use pre-trained).
	â€¢	âœ… Implement webcam + mic capture â†’ save stream (via frontend).
	â€¢	âœ… Set up speech-to-text (Whisper small model) â†’ confirm text output from audio.

ğŸ“Œ Deliverable: You can record yourself, get live transcription & store it in a database.

â¸»

Phase 2: Emotion Detection (Day 3â€“4)

Goal: Detect stress/emotion from audio & video.
	â€¢	âœ… Audio module:
	â€¢	Use pre-trained audio emotion recognition model (SpeechEmotionRecognition / SER).
	â€¢	Output: {happy, sad, angry, neutral, stressed}.
	â€¢	âœ… Video module:
	â€¢	Use MediaPipe or OpenCV + lightweight CNN (FER+ pre-trained) to classify emotions.
	â€¢	Output: {happy, sad, neutral, fear, anger, surprise}.
	â€¢	âœ… Fuse results (rule-based at first):
	â€¢	Example: If both audio & video detect â€œnegative emotionâ€, increase stress score.

ğŸ“Œ Deliverable: When you talk to webcam, dashboard shows live emotion labels.

â¸»

Phase 3: Conversational AI (Day 5â€“6)

Goal: Enable adaptive supportive dialogue.
	â€¢	âœ… Build dialogue engine:
	â€¢	Option A: Rasa/Dialogflow (rule-based + intents).
	â€¢	Option B: Use small LLM (like GPT-2 fine-tuned or distilGPT-2) + predefined prompts.
	â€¢	âœ… Map emotional state â†’ supportive responses.
	â€¢	Example:
	â€¢	If stressed â†’ respond with calming messages, breathing tips.
	â€¢	If neutral â†’ check-in questions.
	â€¢	If positive â†’ encouragement.
	â€¢	âœ… Store logs: emotion + conversation summary in SQLite/Postgres.

ğŸ“Œ Deliverable: System reacts to userâ€™s stress with a supportive, short conversation.

â¸»

Phase 4: Reporting & Dashboard (Day 7)

Goal: Present data for astronauts & ground crew.
	â€¢	âœ… Build simple dashboard (Plotly/Streamlit):
	â€¢	Show emotion trend over session timeline.
	â€¢	Stress alerts triggered.
	â€¢	Conversation logs.
	â€¢	âœ… Export report (PDF/CSV) summarizing key findings.

ğŸ“Œ Deliverable: Ground-control style dashboard where you can see crew well-being trends.

â¸»

Stretch Goals (If Time Allows)
	â€¢	ğŸ”¹ Physical well-being proxy: detect yawning, eye fatigue (via video).
	â€¢	ğŸ”¹ Personalized calibration (baseline 1 min â€œmood scanâ€ per astronaut).
	â€¢	ğŸ”¹ Offline mode (edge deployment on device without internet).
	â€¢	ğŸ”¹ Multi-user monitoring (simulate multiple astronauts).

â¸»

ğŸ“Š Final Hackathon Demo Flow
	1.	Start session â†’ webcam + mic capture.
	2.	System detects speech & emotions in real-time.
	3.	Dashboard shows live stress score & emotion.
	4.	If stress â†‘ â†’ AI assistant starts supportive conversation.
	5.	At end â†’ report generated for ground crew.
